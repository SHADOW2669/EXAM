## Part A

### 1\. a) Define multiprocessor system and mention two advantages of multiprocessor system.

A **multiprocessor system** is a computer system with two or more central processing units (CPUs) that share common access to memory and peripherals. These CPUs can execute multiple instructions simultaneously, leading to increased system performance.

Two advantages of a multiprocessor system are:

  * **Increased Throughput:** More tasks can be executed in a shorter amount of time as multiple CPUs work in parallel.
  * **Improved Reliability/Fault Tolerance:** If one processor fails, the system can often continue to operate, albeit with reduced performance, as other processors can take over the workload.

### b) Give any four File Types.

Four common file types are:

  * **Text Files (.txt):** Contain plain unformatted text.
  * **Executable Files (.exe, .com):** Contain machine code that can be executed by the operating system.
  * **Image Files (.jpg, .png, .gif):** Contain visual data.
  * **Audio Files (.mp3, .wav):** Contain sound data.

### c) What are first-fit and worst-fit memory allocations?

**First-fit memory allocation:** This is a memory allocation algorithm that searches for the first available memory block that is large enough to satisfy the request. It starts searching from the beginning of memory and allocates the first suitable hole it finds.

**Worst-fit memory allocation:** This memory allocation algorithm searches for the largest available memory block and allocates the requested memory from it. The idea is to leave the largest possible hole remaining, hoping it will be useful for future large allocations.

### d) What do you mean by logical addresses and physical addresses?

**Logical Address (Virtual Address):** A logical address is an address generated by the CPU. It is a virtual address that does not directly correspond to the actual physical location in memory. Programs and processes primarily work with logical addresses.

**Physical Address:** A physical address is the actual address in the main memory (RAM). This is the address that the memory management unit (MMU) translates the logical address into, allowing the CPU to access the specific memory location.

### e) Define pre-emptive scheduling and non-pre-emptive scheduling.

**Pre-emptive Scheduling:** In pre-emptive scheduling, the currently executing process can be interrupted and moved to the ready state by the operating system. This interruption can occur due to events like a higher-priority process arriving, a time slice expiring, or an I/O interrupt. This ensures fairness and responsiveness, especially for interactive systems.

**Non-pre-emptive Scheduling:** In non-pre-emptive scheduling, once a process starts executing, it continues to run until it completes its CPU burst or voluntarily yields the CPU (e.g., by requesting I/O). It cannot be interrupted by other processes, even if a higher-priority process becomes ready.

### f) What do you mean by Inter Process Communication (IPC)?

**Inter-Process Communication (IPC)** refers to a set of mechanisms provided by operating systems that allow different processes to communicate and synchronize their actions with each other. This is crucial for cooperative processes that need to share data or coordinate their activities. Common IPC mechanisms include pipes, message queues, shared memory, and semaphores.

### g) Mention the two methods of handling the deadlock.

Two common methods of handling deadlock are:

  * **Deadlock Prevention:** This approach aims to prevent deadlock from occurring in the first place by ensuring that at least one of the four necessary conditions for deadlock (mutual exclusion, hold and wait, no preemption, circular wait) is never met.
  * **Deadlock Avoidance:** This approach allows the necessary conditions for deadlock to exist but imposes restrictions on resource allocation to ensure that the system never enters an unsafe state (a state where deadlock could occur). Banker's Algorithm is a classic example of deadlock avoidance.

### h) What is Semaphore?

A **semaphore** is a synchronization primitive used in concurrent programming to control access to a common resource by multiple processes or threads. It is essentially an integer variable that is accessed through two standard atomic operations: `wait()` (also known as `P()` or `down()`) and `signal()` (also known as `V()` or `up()`). Semaphores are used to solve critical section problems and achieve process synchronization.

## Part B

### Unit - I

### 2\. a) Explain Operating System Resource Management in

#### i) Process Management

#### ii) Memory Management.

**Operating System Resource Management** is a core function of an operating system, responsible for efficiently allocating and deallocating system resources (like CPU time, memory, I/O devices, files) among various competing processes.

#### i) Process Management

Process management is the part of the operating system responsible for managing processes, which are instances of programs in execution. This involves:

  * **Process Creation and Termination:** Creating new processes and terminating existing ones (either normally or abnormally).
  * **Process Scheduling:** Deciding which process gets access to the CPU and for how long. This includes various scheduling algorithms (e.g., FCFS, SJF, Priority, Round Robin).
  * **Process Synchronization:** Providing mechanisms (like semaphores, mutexes, monitors) to ensure that multiple processes can cooperate and access shared resources without leading to inconsistent data.
  * **Deadlock Handling:** Dealing with situations where processes are blocked indefinitely, waiting for resources held by other blocked processes.
  * **Inter-Process Communication (IPC):** Facilitating communication and data exchange between different processes.

#### ii) Memory Management

Memory management is the part of the operating system responsible for managing the computer's primary memory (RAM). Its goals are to efficiently allocate memory to processes, protect processes from each other's memory spaces, and allow programs to run even if they are larger than physical memory. This involves:

  * **Memory Allocation and Deallocation:** Assigning blocks of memory to processes when they are created and reclaiming that memory when processes terminate. This includes techniques like contiguous allocation (first-fit, best-fit, worst-fit) and non-contiguous allocation (paging, segmentation).
  * **Address Mapping/Translation:** Translating logical addresses generated by the CPU into physical addresses in main memory. This is typically done by the Memory Management Unit (MMU).
  * **Memory Protection:** Ensuring that one process cannot access the memory space of another process, preventing data corruption and system crashes.
  * **Swapping:** Temporarily moving processes from main memory to secondary storage (disk) to free up memory for other processes, and bringing them back when needed.
  * **Virtual Memory:** A technique that allows programs to execute even if they are not entirely in physical memory, by using disk space as an extension of RAM. This involves paging and segmentation.

### b) Explain layered approach of operating system structure with a neat diagram.

The **layered approach** to operating system structure organizes the operating system into distinct layers, each built upon the layer below it. Each layer offers specific services to the layer above it and uses the services provided by the layer below it. The lowest layer is the hardware, and the highest layer is the user interface.

**Advantages:**

  * **Modularity:** Each layer is responsible for a specific set of functions, making the OS easier to design, implement, and debug.
  * **Debugging and Maintenance:** Errors can be isolated to specific layers, simplifying the debugging process. Changes in one layer generally do not affect other layers, as long as the interfaces between layers remain consistent.
  * **Abstraction:** Each layer provides an abstract view of the underlying complexity, hiding implementation details from higher layers.

**Disadvantages:**

  * **Performance Overhead:** Each layer adds a certain level of overhead due to the multiple function calls and data transfers between layers, which can reduce performance.
  * **Difficulty in Defining Layers:** It can be challenging to define appropriate layers and assign functionalities to them in a truly independent manner.

**Diagram:**

```
+---------------------+
|                     |
|  Layer N: User Apps |
|                     |
+---------------------+
|                     |
|  Layer N-1: System  |
|             Utilities |
+---------------------+
|                     |
|  Layer N-2: File    |
|             System  |
+---------------------+
|                     |
|  Layer N-3: Memory  |
|             Mgmt.   |
+---------------------+
|                     |
|  Layer N-4: Process |
|             Mgmt.   |
+---------------------+
|                     |
|  Layer N-5: Device  |
|             Drivers |
+---------------------+
|                     |
|  Layer 0: Hardware  |
|                     |
+---------------------+
```

**Explanation of Layers (Example):**

  * **Layer 0 (Hardware):** This is the lowest layer, consisting of the physical components of the computer system (CPU, memory, I/O devices, etc.).
  * **Layer 1 (Device Drivers):** This layer provides the interface to the hardware. It contains device-specific code that controls and manages the various hardware devices.
  * **Layer 2 (Process Management/CPU Scheduling):** This layer manages processes, handles CPU scheduling, and provides context switching.
  * **Layer 3 (Memory Management):** This layer is responsible for memory allocation, deallocation, and virtual memory management.
  * **Layer 4 (File System):** This layer handles file operations, directory management, and storage allocation on secondary storage.
  * **Layer 5 (System Utilities):** This layer includes utilities that provide common services to user applications, such as print spooling, networking, etc.
  * **Layer N (User Applications):** This is the outermost layer where user programs and applications run. They interact with the operating system through system calls provided by the layers below.

### Unit - II

### 3\. a) Explain services of an operating system.

Operating systems provide a wide range of services to both users and applications to make the computer system convenient to use and efficient to operate. Here are some key services:

  * **Program Execution:** The OS loads a program into memory and executes it. It handles program execution from start to finish, including context switching and CPU scheduling.
  * **I/O Operations:** The OS manages input/output devices (keyboard, mouse, printer, disk drives, etc.). It provides a standardized interface for applications to interact with these devices, abstracting away the hardware complexities.
  * **File System Manipulation:** The OS provides services for creating, deleting, reading, writing, and manipulating files and directories. It manages file storage on secondary memory and ensures data integrity.
  * **Communications:** The OS facilitates communication between processes, both on the same computer (Inter-Process Communication - IPC) and across networks.
  * **Error Detection and Response:** The OS constantly monitors the system for potential errors (e.g., memory errors, I/O errors, CPU errors, application errors). It takes appropriate action to ensure system stability, such as displaying error messages, terminating faulty processes, or attempting recovery.
  * **Resource Allocation:** The OS manages and allocates system resources (CPU time, memory, I/O devices, files) to various processes to ensure fair and efficient utilization.
  * **Accounting:** The OS keeps track of which users use how much and what kinds of computer resources. This is useful for billing, performance monitoring, and resource optimization.
  * **Protection and Security:** The OS protects system resources and user data from unauthorized access or malicious activities. It implements access control mechanisms, authentication, and other security features.
  * **User Interface:** The OS provides a user interface (e.g., Command Line Interface (CLI), Graphical User Interface (GUI), Batch Interface) to allow users to interact with the computer system and issue commands.

### b) Write a note on the following :

#### i) File attributes

**File attributes** are metadata associated with a file that describes its characteristics and properties. These attributes are stored by the file system and are used by the operating system and applications to manage and identify files. Common file attributes include:

  * **Name:** The symbolic name of the file, typically unique within its directory.
  * **Type:** Indicates the file format or purpose (e.g., text, executable, image, audio). This is often inferred from the file extension.
  * **Location:** Pointer to the physical location of the file's data on the storage device (e.g., disk blocks, cylinder, track, sector).
  * **Size:** The current size of the file in bytes, blocks, or words.
  * **Protection/Permissions:** Controls who can read, write, or execute the file (e.g., owner, group, others).
  * **Time and Date:**
      * **Creation Time:** When the file was created.
      * **Last Modified Time:** When the file's content was last changed.
      * **Last Accessed Time:** When the file was last read or accessed.
  * **Owner/User ID:** The user who created or owns the file.
  * **Group ID:** The group associated with the file, used for group-based permissions.
  * **Hidden:** A flag indicating whether the file should be visible in standard directory listings.
  * **System:** A flag indicating whether the file is a critical system file.
  * **Archive:** A flag used by backup utilities to indicate if the file has been modified since the last backup.
  * **Read-Only:** A flag indicating that the file cannot be modified or deleted.

These attributes are essential for file system operations, security, data recovery, and user interaction.

#### ii) System calls

**System calls** provide the interface between a running program and the operating system. They are the means by which user-mode processes request services from the kernel (the core of the operating system). When a program needs to perform an operation that requires privileged access or direct interaction with hardware, it makes a system call.

**How they work:**

1.  A user program executes a special instruction (often a software interrupt or trap) that transfers control to a predefined location in the operating system's kernel.
2.  Parameters are passed to the kernel, specifying the desired service and any necessary data.
3.  The kernel verifies the request and executes the privileged instructions required to fulfill the service (e.g., accessing hardware, allocating memory, creating a process).
4.  After completing the service, the kernel returns control to the user program, often with a return value indicating success or failure.

**Examples of System Calls:**

  * `open()`: To open a file.
  * `read()`: To read data from a file or device.
  * `write()`: To write data to a file or device.
  * `fork()`: To create a new process.
  * `exec()`: To load and execute a new program in the current process.
  * `exit()`: To terminate a process.
  * `malloc()`: To allocate memory (though this is often a library function that internally uses system calls).
  * `getpid()`: To get the process ID.
  * `kill()`: To send a signal to a process.

System calls are fundamental to how applications interact with the operating system, allowing them to utilize hardware resources and OS functionalities in a controlled and secure manner.

### 4\. a) Write a short note on fragmentation.

**Fragmentation** is a phenomenon in computer memory management and file systems where memory or disk space becomes divided into many small, non-contiguous blocks, making it difficult to allocate larger contiguous blocks, even if the total free space is sufficient. This can lead to inefficient use of resources and degraded performance.

There are two main types of fragmentation:

  * **Internal Fragmentation:** This occurs when memory is allocated in fixed-size blocks (e.g., in paging or fixed-partition memory allocation), and a process is allocated a block that is larger than its actual memory requirement. The unused portion within the allocated block is wasted and cannot be used by any other process. For example, if memory is allocated in 4KB pages and a process only needs 3KB, 1KB within that page is internally fragmented.

  * **External Fragmentation:** This occurs when there is enough total free space in memory or on disk to satisfy a request, but the free space is scattered in many small, non-contiguous blocks. These small holes are too small individually to satisfy the request, even though their sum would be sufficient. This is common in variable-partition memory allocation and dynamic disk allocation. For example, if you have 10KB of free space, but it's split into five 2KB blocks separated by allocated blocks, you cannot allocate a 5KB block.

**Consequences of Fragmentation:**

  * **Reduced Memory Utilization:** Wasted space due to internal or external fragmentation.
  * **Decreased Performance:** The operating system may spend more time searching for suitable memory blocks or performing compaction.
  * **Inability to Allocate Large Blocks:** Even with sufficient total free space, large requests may fail if no single contiguous block is large enough.

**Solutions to Fragmentation:**

  * **Compaction:** (Primarily for external fragmentation) Shuffling memory contents to bring all free space together into one large block. This is often computationally expensive.
  * **Paging/Segmentation:** (For memory management) Techniques that allow a process to be stored in non-contiguous memory locations, reducing external fragmentation.
  * **Variable-Sized Blocks/Dynamic Allocation Algorithms:** (For disk) Intelligently allocating space to minimize wasted space.

### b) Consider the following page reference string

`7, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1`

How many page faults would occur for the following replacement algorithm assuming three frames ?

#### i) LRU algorithm

#### ii) Optimal replacement algorithm.

Let's assume three frames for both scenarios.

Page Reference String: `7, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1`

#### i) LRU (Least Recently Used) algorithm

The LRU algorithm replaces the page that has not been used for the longest period of time.

| Reference | Frame 1 | Frame 2 | Frame 3 | Page Fault | Notes                                   |
| --------- | ------- | ------- | ------- | ---------- | --------------------------------------- |
| 7         | 7       |         |         | Yes        | Frame 1 gets 7                          |
| 0         | 7       | 0       |         | Yes        | Frame 2 gets 0                          |
| 3         | 7       | 0       | 3       | Yes        | Frame 3 gets 3                          |
| 0         | 7       | 0       | 3       | No         | 0 is already in memory                  |
| 4         | 4       | 0       | 3       | Yes        | 7 is LRU, replace 7 with 4              |
| 2         | 4       | 2       | 3       | Yes        | 0 is LRU, replace 0 with 2              |
| 3         | 4       | 2       | 3       | No         | 3 is already in memory                  |
| 0         | 0       | 2       | 3       | Yes        | 4 is LRU, replace 4 with 0              |
| 3         | 0       | 2       | 3       | No         | 3 is already in memory                  |
| 2         | 0       | 2       | 3       | No         | 2 is already in memory                  |
| 1         | 0       | 1       | 3       | Yes        | 2 is LRU, replace 2 with 1              |
| 2         | 0       | 1       | 2       | Yes        | 3 is LRU, replace 3 with 2              |
| 0         | 0       | 1       | 2       | No         | 0 is already in memory                  |
| 1         | 0       | 1       | 2       | No         | 1 is already in memory                  |
| 7         | 7       | 1       | 2       | Yes        | 0 is LRU, replace 0 with 7              |
| 0         | 7       | 0       | 2       | Yes        | 1 is LRU, replace 1 with 0              |
| 1         | 7       | 0       | 1       | Yes        | 2 is LRU, replace 2 with 1              |

Total page faults for LRU: **12**

#### ii) Optimal replacement algorithm

The Optimal algorithm replaces the page that will not be used for the longest period of time in the future. This is an ideal algorithm and is not practically implementable, but it serves as a benchmark.

| Reference | Frame 1 | Frame 2 | Frame 3 | Page Fault | Notes                                             |
| --------- | ------- | ------- | ------- | ---------- | ------------------------------------------------- |
| 7         | 7       |         |         | Yes        | Frame 1 gets 7                                    |
| 0         | 7       | 0       |         | Yes        | Frame 2 gets 0                                    |
| 3         | 7       | 0       | 3       | Yes        | Frame 3 gets 3                                    |
| 0         | 7       | 0       | 3       | No         | 0 is already in memory                            |
| 4         | 4       | 0       | 3       | Yes        | 7 is used furthest in future, replace 7 with 4    |
| 2         | 4       | 0       | 2       | Yes        | 3 is used furthest in future, replace 3 with 2    |
| 3         | 4       | 0       | 3       | Yes        | 2 is used furthest in future, replace 2 with 3    |
| 0         | 4       | 0       | 3       | No         | 0 is already in memory                            |
| 3         | 4       | 0       | 3       | No         | 3 is already in memory                            |
| 2         | 2       | 0       | 3       | Yes        | 4 is used furthest in future, replace 4 with 2    |
| 1         | 2       | 1       | 3       | Yes        | 0 is used furthest in future, replace 0 with 1    |
| 2         | 2       | 1       | 3       | No         | 2 is already in memory                            |
| 0         | 2       | 0       | 3       | Yes        | 1 is used furthest in future, replace 1 with 0    |
| 1         | 2       | 1       | 3       | Yes        | 0 is used furthest in future, replace 0 with 1    |
| 7         | 2       | 1       | 7       | Yes        | 3 is used furthest in future, replace 3 with 7    |
| 0         | 0       | 1       | 7       | Yes        | 2 is used furthest in future, replace 2 with 0    |
| 1         | 0       | 1       | 7       | No         | 1 is already in memory                            |

Total page faults for Optimal: **13**

**Note:** It appears there might be an error in my trace or understanding of "optimal" for one of the steps. Let me re-verify the Optimal trace carefully, as Optimal should generally have *fewer* or equal page faults compared to LRU.

Let's re-trace Optimal for clarity.

Page Reference String: `7, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1`
Frames: 3

| Reference | Frames (F1, F2, F3) | Page Fault | Pages in Frames | Next Use of Pages in Frames | Page to Replace |
| --------- | ------------------- | ---------- | --------------- | --------------------------- | --------------- |
| 7         | (7, -, -)           | Yes        | {7}             | 7: (end)                    |                 |
| 0         | (7, 0, -)           | Yes        | {7, 0}          | 7: (end), 0: 4              |                 |
| 3         | (7, 0, 3)           | Yes        | {7, 0, 3}       | 7: (end), 0: 4, 3: 6        |                 |
| 0         | (7, 0, 3)           | No         | {7, 0, 3}       | 7: (end), 0: 4, 3: 6        |                 |
| 4         | (4, 0, 3)           | Yes        | {4, 0, 3}       | 4: (end), 0: 4, 3: 6        | Replace 7 (no future use) |
| 2         | (4, 0, 2)           | Yes        | {4, 0, 2}       | 4: (end), 0: 4, 2: 6        | Replace 3 (used at 6, 2 used at 6, 0 at 9). No, 3 is used at index 6, 2 at index 10. Let's list future uses: 4(end), 0(9), 2(10), 3(6). The one to be replaced is 3. |
| 3         | (4, 0, 3)           | Yes        | {4, 0, 3}       | 4: (end), 0: 9, 3: (end)    | Replace 2 (no future use) |
| 0         | (4, 0, 3)           | No         | {4, 0, 3}       | 4: (end), 0: 9, 3: (end)    |                 |
| 3         | (4, 0, 3)           | No         | {4, 0, 3}       | 4: (end), 0: 9, 3: (end)    |                 |
| 2         | (2, 0, 3)           | Yes        | {2, 0, 3}       | 2: (end), 0: 9, 3: (end)    | Replace 4 (no future use) |
| 1         | (2, 1, 3)           | Yes        | {2, 1, 3}       | 2: (end), 1: 13, 3: (end)   | Replace 0 (no future use) |
| 2         | (2, 1, 3)           | No         | {2, 1, 3}       | 2: (end), 1: 13, 3: (end)   |                 |
| 0         | (0, 1, 3)           | Yes        | {0, 1, 3}       | 0: 16, 1: 13, 3: (end)      | Replace 2 (no future use) |
| 1         | (0, 1, 3)           | No         | {0, 1, 3}       | 0: 16, 1: 13, 3: (end)      |                 |
| 7         | (0, 1, 7)           | Yes        | {0, 1, 7}       | 0: 16, 1: (end), 7: (end)   | Replace 3 (no future use) |
| 0         | (0, 1, 7)           | No         | {0, 1, 7}       | 0: 16, 1: (end), 7: (end)   |                 |
| 1         | (0, 1, 7)           | No         | {0, 1, 7}       | 0: 16, 1: (end), 7: (end)   |                 |

Total page faults for Optimal: **10**

My previous manual trace for Optimal had an error. The corrected count for Optimal is **10**.

### c) Explain SCAN disk scheduling with example.

**SCAN (Elevator) Disk Scheduling Algorithm:**

The SCAN disk scheduling algorithm operates like an elevator. The disk arm starts at one end of the disk and moves towards the other end, servicing all requests in its path. Once it reaches the other end, it reverses its direction and again services all requests in its path. This process continues, moving back and forth across the disk.

**Characteristics:**

  * **Fairness:** It provides more uniform wait times than FCFS.
  * **Starvation:** Pages at the extreme ends of the disk might experience long wait times if the requests are mostly concentrated in the middle.
  * **Direction Bias:** It favors requests in the current direction of the disk arm.

**Example:**

Consider a disk with 200 cylinders (0 to 199).
Current head position: 53
Requests queue: `98, 183, 37, 122, 14, 124, 65, 67`

Let's assume the disk arm is initially moving towards the higher cylinder numbers.

1.  **Start at 53, moving towards 199:**

      * **53 -\> 65:** (12 seek time)
      * **65 -\> 67:** (2 seek time)
      * **67 -\> 98:** (31 seek time)
      * **98 -\> 122:** (24 seek time)
      * **122 -\> 124:** (2 seek time)
      * **124 -\> 183:** (59 seek time)
      * **183 -\> 199 (end of disk):** (16 seek time) - The arm moves to the end even if there are no requests there, to reverse direction.

2.  **Reverse direction, moving towards 0 from 199:**

      * **199 -\> 37:** (162 seek time) - This covers the remaining requests: 37, 14.
      * **37 -\> 14:** (23 seek time)

**Sequence of serviced requests:** `53 -> 65 -> 67 -> 98 -> 122 -> 124 -> 183 -> 199 -> 37 -> 14`

**Total Head Movement:**
(65-53) + (67-65) + (98-67) + (122-98) + (124-122) + (183-124) + (199-183) + (199-37) + (37-14)
\= 12 + 2 + 31 + 24 + 2 + 59 + 16 + 162 + 23 = **331 cylinders**

### 5\. a) Explain Contiguous Memory Allocation.

**Contiguous memory allocation** is a memory management technique where each process is allocated a single, continuous block of memory. This means that all parts of a process must reside in a single, unbroken section of physical memory.

**How it works:**

When a process requests memory, the operating system searches for a free block (or "hole") in the main memory that is large enough to accommodate the entire process. Once such a block is found, it is allocated to the process, and the process is loaded into that contiguous space.

**Types of Contiguous Allocation:**

1.  **Fixed-Partition Allocation (Static Partitioning):**

      * Memory is divided into a fixed number of partitions at system startup.
      * Each partition has a predefined size.
      * A process can be loaded into any partition that is large enough.
      * **Disadvantages:**
          * **Internal Fragmentation:** If a process is smaller than the partition, the remaining space within the partition is wasted.
          * **Limited Multiprogramming:** The number of processes that can reside in memory is limited by the number of partitions.
          * **Inefficient Use of Large Processes:** A large process might not fit into any available partition.

2.  **Variable-Partition Allocation (Dynamic Partitioning):**

      * Memory is initially one large free block.
      * When a process arrives, the OS allocates exactly the amount of memory it needs from a free hole.
      * When a process terminates, its memory block is returned to the pool of free memory, potentially merging with adjacent free blocks.
      * **Disadvantages:**
          * **External Fragmentation:** As processes are loaded and unloaded, memory can become fragmented into many small, non-contiguous holes, even if the total free space is sufficient for a new process. This is the primary problem.
          * **Compaction Overhead:** To combat external fragmentation, memory compaction might be needed, which is a costly process of moving processes to consolidate free space.

**Advantages of Contiguous Memory Allocation (in general):**

  * **Simplicity:** It's relatively easy to implement and manage compared to non-contiguous methods.
  * **Fast Access:** Once a process is loaded, accessing its memory locations is straightforward and fast because all its parts are physically adjacent.

**Overall, while simple, contiguous memory allocation suffers significantly from fragmentation, particularly external fragmentation in dynamic partitioning, leading to inefficient memory utilization.** This led to the development of non-contiguous memory management techniques like paging and segmentation.

### b) Explain FIFO page replacement algorithm with an example.

**FIFO (First-In, First-Out) Page Replacement Algorithm:**

The FIFO page replacement algorithm is one of the simplest page replacement algorithms. It works on the principle that the page that has been in memory for the longest time (i.e., the first page to enter the memory) is the one to be replaced when a new page needs to be brought in and all frames are full. It treats the page frames as a circular buffer or a queue.

**How it works:**

1.  When a page is referenced:
      * If the page is already in memory (a **hit**), no action is taken.
      * If the page is not in memory (a **page fault**):
          * If there are empty frames, the new page is loaded into an empty frame.
          * If all frames are full, the oldest page (the one that entered memory first) is replaced by the new page.

**Example:**

Consider a page reference string: `7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1`
Number of frames: 3

| Reference | Frame 1 | Frame 2 | Frame 3 | Page Fault | Notes                                |
| --------- | ------- | ------- | ------- | ---------- | ------------------------------------ |
| 7         | 7       |         |         | Yes        | 7 loaded                             |
| 0         | 7       | 0       |         | Yes        | 0 loaded                             |
| 1         | 7       | 0       | 1       | Yes        | 1 loaded                             |
| 2         | 2       | 0       | 1       | Yes        | 7 is oldest, replace with 2          |
| 0         | 2       | 0       | 1       | No         | 0 already in frame                   |
| 3         | 2       | 3       | 1       | Yes        | 0 is oldest (after previous), replace with 3 |
| 0         | 2       | 3       | 0       | Yes        | 1 is oldest, replace with 0          |
| 4         | 4       | 3       | 0       | Yes        | 2 is oldest, replace with 4          |
| 2         | 4       | 2       | 0       | Yes        | 3 is oldest, replace with 2          |
| 3         | 4       | 2       | 3       | Yes        | 0 is oldest, replace with 3          |
| 0         | 0       | 2       | 3       | Yes        | 4 is oldest, replace with 0          |
| 3         | 0       | 2       | 3       | No         | 3 already in frame                   |
| 2         | 0       | 2       | 3       | No         | 2 already in frame                   |
| 1         | 0       | 1       | 3       | Yes        | 2 is oldest, replace with 1          |
| 2         | 2       | 1       | 3       | Yes        | 0 is oldest, replace with 2          |
| 0         | 2       | 0       | 3       | Yes        | 1 is oldest, replace with 0          |
| 1         | 2       | 0       | 1       | Yes        | 3 is oldest, replace with 1          |
| 7         | 7       | 0       | 1       | Yes        | 2 is oldest, replace with 7          |
| 0         | 7       | 0       | 1       | No         | 0 already in frame                   |
| 1         | 7       | 0       | 1       | No         | 1 already in frame                   |

Total page faults for FIFO: **15**

**Disadvantages of FIFO:**

  * **Belady's Anomaly:** Increasing the number of page frames can sometimes *increase* the number of page faults, which is counter-intuitive. This is a significant drawback of FIFO.
  * **Ignores Locality:** FIFO does not consider the frequency or recency of page use, meaning frequently used pages can be replaced if they happen to be the oldest.

### c) Explain swapping with diagram.

**Swapping** is a memory management technique used by operating systems to temporarily move a process or a part of a process from main memory (RAM) to secondary storage (disk) and vice versa. The purpose of swapping is to allow more processes to be loaded into memory than physically available at any given time, thereby increasing the degree of multiprogramming.

**Why Swapping is Needed:**

  * **Multiprogramming:** To run more processes concurrently than can fit entirely in main memory.
  * **Memory Pressure:** When the demand for physical memory exceeds its availability.
  * **Process Creation:** To make room for new processes.

**How Swapping Works:**

1.  **Swap Out (Roll Out):** When the operating system needs to free up main memory (e.g., a higher-priority process needs to run, or memory is low), it selects a low-priority or idle process, moves its entire memory image from RAM to a designated area on the hard disk called the **swap space** (or **swap partition/file**). The process's entry in the process control block (PCB) is updated to indicate it's swapped out.
2.  **Swap In (Roll In):** When the swapped-out process is needed again (e.g., its turn to run, or a high-priority process finishes), its memory image is moved back from the swap space on disk to main memory.

**Diagram:**

```
+------------------------+      +------------------------+
|                        |      |                        |
|   Main Memory (RAM)    |      |    Secondary Storage   |
|                        |      |        (Disk)          |
| +--------------------+ |      | +--------------------+ |
| |  OS Kernel         | |      | |  Swap Space        | |
| +--------------------+ |      | | (Swap Partition/File)| |
| |  Process A         | |      | |                    | |
| +--------------------+ |      | |                    | |
| |  Process B         | |      | |                    | |
| +--------------------+ |      | |                    | |
| |  Free Space        | |      | |                    | |
| +--------------------+ |      | +--------------------+ |
|                        |      |                        |
+------------------------+      +------------------------+
          ^        |
          |        | (Swap Out)
          |        V
    (Swap In)
```

**Scenario:**

Imagine Main Memory has Process A and Process B, and limited free space. If Process C arrives and needs to be executed, but there isn't enough contiguous free space, the OS might decide to **swap out** Process B to the swap space on the disk. This creates a larger free block in Main Memory. Then, Process C can be **swapped in** from disk into the newly freed space. When Process B is needed again, Process C (or another process) might be swapped out to make room for Process B to be swapped back in.

**Considerations:**

  * **Overhead:** Swapping involves significant I/O operations (reading/writing to disk), which are much slower than memory access. Excessive swapping (thrashing) can severely degrade system performance.
  * **Granularity:** Swapping typically moves entire processes. Paging, a more refined technique, moves individual pages, offering finer control and reducing external fragmentation.
  * **Demand Paging:** Modern operating systems primarily use demand paging as a more efficient form of virtual memory, where pages are swapped in and out only when they are actually referenced.

### Unit - III

### 6\. a) What is Process ? Explain process state transition with neat diagram.

A **process** is an instance of a computer program that is being executed. It is not just the program code itself, but also includes its current activity, program counter, registers, stack, data section, and resources allocated to it. In simple terms, a program is a passive entity (a file on disk), while a process is an active entity (a program in execution).

**Process State Transition:**

A process goes through various states during its lifetime as it executes and interacts with the operating system. The transitions between these states are managed by the operating system's process scheduler.

**Process States:**

1.  **New:** The process is being created. It has not yet been admitted to the pool of executable processes.
2.  **Ready:** The process is waiting to be assigned to a processor. It is ready to execute but is waiting for its turn on the CPU.
3.  **Running:** The process is currently executing instructions on the CPU.
4.  **Waiting (Blocked):** The process is waiting for some event to occur, such as the completion of an I/O operation, the availability of a resource, or a signal from another process. It cannot proceed until the event occurs.
5.  **Terminated (Halted):** The process has finished execution, either normally or abnormally. It is no longer active but may still exist in the system's process table for a short period to allow for cleanup or status reporting.

**Process State Transition Diagram:**

```
             +------------+
             |            |
             |    New     |
             |            |
             +------+-----+
                    |
                    | Admission (OS admits process)
                    V
             +------+-----+
             |            |
             |    Ready   |<-------------------+
             |            |                    |
             +------+-----+                    |
                    | Dispatch (Scheduler selects)
                    V                          | Event Wait (e.g., I/O complete)
             +------+-----+                    |
             |            |                    |
             |   Running  |--------------------+
             |            |      Timeout/Interrupt (e.g., Time slice expired, Higher Priority)
             +------+-----+
           /          \
          /            \
         /              \
        |                |
        | Event Wait     | Exit (Process completes/aborts)
        | (e.g., I/O Req) |
        V                V
  +------+-----+      +------------+
  |            |      |            |
  |   Waiting  |----->| Terminated |
  | (Blocked)  | Event|            |
  +------------+      +------------+
```

**Explanation of Transitions:**

  * **New -\> Ready:** When the operating system creates a new process, it moves from the "New" state to the "Ready" state, indicating it's ready to run and waiting for the CPU.
  * **Ready -\> Running:** The CPU scheduler selects one of the processes from the "Ready" queue and dispatches it to the CPU. The process then enters the "Running" state.
  * **Running -\> Ready:**
      * **Time Slice Expired:** In pre-emptive scheduling, if the process's allocated time quantum (time slice) expires, it is moved back to the "Ready" queue.
      * **Interrupt:** An interrupt (e.g., a hardware interrupt, a timer interrupt) can cause the running process to be preempted and moved to the "Ready" state.
      * **Higher Priority Process:** If a higher-priority process becomes ready, the running process might be preempted.
  * **Running -\> Waiting (Blocked):** The process moves to the "Waiting" state if it needs to wait for some event to occur, such as:
      * Requesting I/O (e.g., reading from disk, printing).
      * Waiting for a child process to terminate.
      * Waiting for a resource to become available.
  * **Waiting (Blocked) -\> Ready:** Once the event for which the process was waiting occurs (e.g., I/O completion, resource becomes available), the process is moved from the "Waiting" state back to the "Ready" state, where it can again contend for the CPU.
  * **Running -\> Terminated:** The process enters the "Terminated" state when it completes its execution normally (e.g., `exit()` call) or is abnormally terminated (e.g., by an error, or killed by the OS).

### b) Consider the following set of processes, with length of the CPU-burst time given in milliseconds.

| Process | CPU Burst Time |
| :------ | :------------- |
| P1      | 24             |
| P2      | 3              |
| P3      | 3              |

Find the average turnaround and waiting time for FIFO scheduling algorithm and SJF scheduling algorithm. And also draw the Gantt chart for FIFO and SJF scheduling algorithm.

#### FIFO (First-In, First-Out) Scheduling Algorithm

The FIFO algorithm executes processes in the order they arrive. We assume the arrival order is P1, P2, P3.

**Gantt Chart (FIFO):**

```
+----+----+----+
| P1 | P2 | P3 |
+----+----+----+
0    24   27   30
```

**Calculations for FIFO:**

  * **P1:**
      * Completion Time: 24
      * Turnaround Time (CT - AT, assuming AT=0): 24 - 0 = 24
      * Waiting Time (Turnaround Time - Burst Time): 24 - 24 = 0
  * **P2:**
      * Completion Time: 27
      * Turnaround Time: 27 - 0 = 27
      * Waiting Time: 27 - 3 = 24
  * **P3:**
      * Completion Time: 30
      * Turnaround Time: 30 - 0 = 30
      * Waiting Time: 30 - 3 = 27

**Average Turnaround Time (FIFO):** $(24 + 27 + 30) / 3 = 81 / 3 = \\textbf{27 ms}$
**Average Waiting Time (FIFO):** $(0 + 24 + 27) / 3 = 51 / 3 = \\textbf{17 ms}$

#### SJF (Shortest Job First) Scheduling Algorithm

SJF is a non-preemptive algorithm that selects the process with the smallest CPU burst time next.

Processes and Burst Times:

  * P1: 24
  * P2: 3
  * P3: 3

Order of execution (shortest burst first): P2, P3, P1 (if burst times are equal, FIFO order is usually assumed, so P2 before P3).

**Gantt Chart (SJF):**

```
+----+----+----+
| P2 | P3 | P1 |
+----+----+----+
0    3    6    30
```

**Calculations for SJF:**

  * **P2:**
      * Completion Time: 3
      * Turnaround Time: 3 - 0 = 3
      * Waiting Time: 3 - 3 = 0
  * **P3:**
      * Completion Time: 6
      * Turnaround Time: 6 - 0 = 6
      * Waiting Time: 6 - 3 = 3
  * **P1:**
      * Completion Time: 30
      * Turnaround Time: 30 - 0 = 30
      * Waiting Time: 30 - 24 = 6

**Average Turnaround Time (SJF):** $(3 + 6 + 30) / 3 = 39 / 3 = \\textbf{13 ms}$
**Average Waiting Time (SJF):** $(0 + 3 + 6) / 3 = 9 / 3 = \\textbf{3 ms}$

### Unit - IV

### 7\. a) What is PCB ? Explain using appropriate diagram.

A **Process Control Block (PCB)**, also known as a Task Control Block (TCB), is a data structure maintained by the operating system for each process. It contains all the information about a process that is needed by the operating system to manage and control it. Essentially, it's the repository of all information needed to restart a process from where it was interrupted.

When a process is created, the OS creates a PCB for it. When a process context switch occurs (i.e., the OS switches from one process to another), the state of the currently running process is saved into its PCB, and the state of the next process to be run is loaded from its PCB.

**Information Stored in a PCB:**

The PCB typically contains the following categories of information:

1.  **Process State:** The current state of the process (e.g., New, Ready, Running, Waiting, Terminated).
2.  **Program Counter (PC):** The address of the next instruction to be executed for this process.
3.  **CPU Registers:** The values of all CPU registers (general-purpose registers, index registers, stack pointers, etc.) when the process was last interrupted. These are essential for restoring the process's execution context.
4.  **CPU Scheduling Information:**
      * Process priority.
      * Pointers to scheduling queues (e.g., ready queue, device queues).
      * Other scheduling parameters.
5.  **Memory Management Information:**
      * Base and limit registers (for contiguous allocation).
      * Page tables or segment tables (for paging/segmentation).
      * Information about memory allocated to the process.
6.  **Accounting Information:**
      * CPU usage time.
      * Real time used.
      * Time limits.
      * Process ID (PID) and Parent Process ID (PPID).
7.  **I/O Status Information:**
      * List of I/O devices allocated to the process.
      * List of open files.
      * Outstanding I/O requests.
8.  **Process ID (PID):** A unique identifier for the process.
9.  **Parent Process ID (PPID):** The ID of the process that created this process.

**Diagram of PCB and Process Context Switching:**

```
+-------------------------------------------------+
|                                                 |
|          Operating System (Kernel)              |
|                                                 |
| +---------------------------------------------+ |
| |                                             | |
| |  Process Table (Array of PCBs)              | |
| |                                             | |
| |  +----------+    +----------+    +----------+ |
| |  | PCB 1    |--->| PCB 2    |--->| PCB 3    | |
| |  | (P1 Info)|    | (P2 Info)|    | (P3 Info)| |
| |  +----------+    +----------+    +----------+ |
| |      ^                                     | |
| |      |  (Current Process Pointer)          | |
| |      |                                     | |
| +------+-------------------------------------+ |
|        |                                       |
+--------+---------------------------------------+
         |
         | Context Switch (Save Current State / Load New State)
         V
+---------------------+
|                     |
|        CPU          |
|  (Registers, PC)    |
|                     |
+---------------------+
         ^
         |
         |
         |
+---------------------+
|                     |
|     User Space      |
|  (Process P_N Code/Data) |
|                     |
+---------------------+
```

**Explanation:**

  * The Operating System maintains a **Process Table**, which is essentially a collection of PCBs.
  * When a process is running, its state (CPU registers, program counter, etc.) is active in the CPU.
  * When a **context switch** occurs (e.g., due to a time slice expiring, an interrupt, or an I/O request):
    1.  The OS saves the current state of the running process (all CPU register values, PC, etc.) into its corresponding PCB in the Process Table.
    2.  The OS then loads the state of the next process to be executed from its PCB into the CPU registers, effectively restoring its execution context.
  * This mechanism allows the OS to seamlessly switch between multiple processes, giving the illusion that all processes are running concurrently.

### b) Explain Shortest Job First Scheduling algorithm with suitable example.

**Shortest Job First (SJF) Scheduling Algorithm:**

The Shortest Job First (SJF) scheduling algorithm is a non-preemptive CPU scheduling algorithm that schedules processes based on their next CPU burst time. The idea is to execute the process that has the smallest CPU burst time among all the available processes in the ready queue.

**Characteristics:**

  * **Optimality:** SJF is optimal in terms of minimizing the average waiting time for a given set of processes.
  * **Starvation:** It can lead to starvation for long processes if there is a continuous stream of short processes.
  * **Impracticality:** It's difficult to implement in real-time systems because the CPU burst time of a process is usually not known in advance. However, it can be approximated by predicting the next CPU burst using exponential averaging of past bursts.

**Types of SJF:**

  * **Non-preemptive SJF:** Once a process starts executing, it runs to completion, even if a new process with a shorter burst time arrives.
  * **Preemptive SJF (Shortest-Remaining-Time-First - SRTF):** If a new process arrives with a CPU burst time shorter than the remaining time of the currently executing process, the current process is preempted, and the new process is started.

**Example (Non-preemptive SJF):**

Consider the following processes with their arrival times and CPU burst times:

| Process | Arrival Time (ms) | Burst Time (ms) |
| :------ | :---------------- | :-------------- |
| P1      | 0                 | 7               |
| P2      | 2                 | 4               |
| P3      | 4                 | 1               |
| P4      | 5                 | 4               |

**Gantt Chart (Non-preemptive SJF):**

At time 0, only P1 is available. So P1 starts.
At time 2, P2 arrives. P1 is still running.
At time 4, P3 arrives. P1 is still running.
At time 5, P4 arrives. P1 is still running.

P1 completes at 7. Now, from the processes P2, P3, P4 that have arrived, we choose the one with the shortest burst time.
Available: P2 (burst 4), P3 (burst 1), P4 (burst 4).
Shortest is P3 (burst 1). So P3 runs next.

P3 completes at 8. Now, from P2 and P4, if burst times are equal (both 4), we typically choose based on arrival time (P2 arrived at 2, P4 at 5), so P2 runs next.

P2 completes at 12. Only P4 remains.

P4 completes at 16.

```
+----+----+----+----+
| P1 | P3 | P2 | P4 |
+----+----+----+----+
0    7    8    12   16
```

**Calculations for Non-preemptive SJF:**

  * **P1:**
      * Completion Time: 7
      * Turnaround Time: 7 - 0 = 7
      * Waiting Time: 7 - 7 = 0
  * **P2:**
      * Completion Time: 12
      * Turnaround Time: 12 - 2 = 10
      * Waiting Time: 10 - 4 = 6
  * **P3:**
      * Completion Time: 8
      * Turnaround Time: 8 - 4 = 4
      * Waiting Time: 4 - 1 = 3
  * **P4:**
      * Completion Time: 16
      * Turnaround Time: 16 - 5 = 11
      * Waiting Time: 11 - 4 = 7

**Average Turnaround Time:** $(7 + 10 + 4 + 11) / 4 = 32 / 4 = \\textbf{8 ms}$
**Average Waiting Time:** $(0 + 6 + 3 + 7) / 4 = 16 / 4 = \\textbf{4 ms}$

### 8\. a) What is critical section problem ? Explain. What are the requirements for a solution to critical section problem ?

**Critical Section Problem:**

In concurrent programming, the **critical section problem** arises when multiple processes or threads need to access and modify shared resources or data concurrently. A **critical section** is a segment of code where shared resources (like shared variables, files, or hardware devices) are accessed. If multiple processes execute their critical sections simultaneously, it can lead to data inconsistency and race conditions.

The critical section problem is to design a protocol that the processes can use to cooperate so that when one process is executing in its critical section, no other process is allowed to execute in its critical section.

**Example:**

Consider two processes, P1 and P2, both trying to increment a shared counter variable.

```
// Shared variable
int counter = 0;

// Process P1
function process1() {
    // ... other code ...
    // Critical Section
    counter = counter + 1;
    // ... other code ...
}

// Process P2
function process2() {
    // ... other code ...
    // Critical Section
    counter = counter + 1;
    // ... other code ...
}
```

If P1 reads `counter` (say, 5), then P2 also reads `counter` (still 5), then P1 increments `counter` to 6 and writes it back, and P2 increments `counter` to 6 and writes it back, the final value of `counter` will be 6 instead of the expected 7. This is a race condition.

The solution to the critical section problem must ensure that only one process can be inside its critical section at any given time.

**Requirements for a Solution to the Critical Section Problem:**

Any satisfactory solution to the critical section problem must satisfy three essential requirements:

1.  **Mutual Exclusion:** If process $P\_i$ is executing in its critical section, then no other process is allowed to be executing in its critical section. This is the fundamental requirement.
2.  **Progress:** If no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder sections can participate in the decision on which will enter its critical section next, and this selection cannot be postponed indefinitely. This prevents deadlocks and ensures that processes are not indefinitely blocked if the critical section is free.
3.  **Bounded Waiting:** There must be a limit on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section and before that request is granted. This prevents starvation, ensuring that a process waiting to enter its critical section will eventually get its turn.

Common solutions to the critical section problem include mutex locks, semaphores, monitors, and various hardware-based solutions like test-and-set instructions.

### b) Explain resource allocation graph using suitable example.

A **resource-allocation graph** is a directed graph used to describe deadlocks. It provides a visual representation of which processes are holding which resources and which processes are waiting for which resources. This graph is particularly useful for detecting deadlocks in systems with multiple instances of each resource type.

**Components of the Resource-Allocation Graph:**

  * **Nodes:**
      * **Processes ($P\_i$):** Represented by circles.
      * **Resource Types ($R\_j$):** Represented by rectangles.
  * **Edges:**
      * **Request Edge (Process to Resource):** A directed edge from a process $P\_i$ to a resource type $R\_j$ ($P\_i \\rightarrow R\_j$) indicates that process $P\_i$ has requested an instance of resource type $R\_j$ and is currently waiting for it.
      * **Assignment Edge (Resource to Process):** A directed edge from a resource type $R\_j$ to a process $P\_i$ ($R\_j \\rightarrow P\_i$) indicates that an instance of resource type $R\_j$ has been allocated to process $P\_i$.

**Dots/Instances within Resource Rectangles:**

Each dot within a resource rectangle represents an instance of that resource type. For example, if a resource type $R\_1$ has 3 instances, there will be 3 dots inside its rectangle. An assignment edge from a resource type to a process originates from one of these dots, indicating that one instance of that resource type has been allocated.

**Deadlock Detection using Resource-Allocation Graph:**

  * **No Cycles, No Deadlock:** If the resource-allocation graph contains no cycles, then there is no deadlock in the system.
  * **Cycles, Potential Deadlock (if multiple instances of a resource type exist):** If the graph contains a cycle, and each resource type involved in the cycle has only one instance, then a deadlock exists.
  * **Cycles, Deadlock (if multiple instances of a resource type exist):** If the graph contains a cycle, and at least one resource type involved in the cycle has multiple instances, then a deadlock *may* exist. Further analysis (e.g., using a banker's algorithm or a safety algorithm) is needed to confirm if it's a deadlock or just a possibility.

**Example:**

Consider a system with three processes ($P\_1, P\_2, P\_3$) and two resource types ($R\_1, R\_2$).

  * Resource $R\_1$ has one instance.
  * Resource $R\_2$ has two instances.

**Initial State:**

  * $P\_1$ holds an instance of $R\_1$.
  * $P\_2$ holds one instance of $R\_2$.
  * $P\_3$ holds the other instance of $R\_2$.

**Requests:**

  * $P\_1$ requests an instance of $R\_2$.
  * $P\_2$ requests an instance of $R\_1$.

**Resource-Allocation Graph:**

```
+-----+                     +-----+
|  R1 |                     |  R2 |
| (•) |                     | (•)(•) |
+--^--+                     +--^--+
   | R1->P1                 | R2->P2
   |                         | R2->P3
   |                         |
+--+--+                     +--+--+
|  P1 |                     |  P2 |
+-----+                     +-----+
   |                         |
   | P1->R2 (request)        | P2->R1 (request)
   V                         V
+-----+                     +-----+
|  P3 |                     |  .  |
+-----+                     +-----+
```

Let's draw this more clearly:

```
        Request Edge (P -> R)
        Assignment Edge (R -> P)

       P1 ------------> R1 (holds)
       ^                 |
       |                 | (one instance of R1)
       |                 V
       |                 .
       |                 |
       |                 |
       |                 | P1 (requests)
       |                 |
       |                 V
       |                 R2 (two instances of R2)
       |                 /\
       |                /  \
       |               .    .
       |              /      \
       |             V        V
       |            P2        P3
       |           (holds R2) (holds R2)
       |             ^
       |             | P2 (requests)
       |             |
       +-------------+
```

Let's redraw to clearly show the edges.

```mermaid
graph TD
    P1 --> R1_req;
    R1_inst --> P1; // P1 holds R1_instance1

    P2 --> R2_req;
    R2_inst1 --> P2; // P2 holds R2_instance1

    P3 --> R2_req_other;
    R2_inst2 --> P3; // P3 holds R2_instance2

    P1_req_R2 --> R2_req; // P1 requests R2
    P2_req_R1 --> R1_req; // P2 requests R1

    subgraph Resource Types
        R1[R1 (1 instance)]
        R2[R2 (2 instances)]
    end

    subgraph Processes
        P1[P1]
        P2[P2]
        P3[P3]
    end

    // Actual edges based on the example
    R1 --> P1_holds;
    P1_holds --> P1; // P1 holds R1
    P1 --> R2_req;

    R2 --> P2_holds;
    P2_holds --> P2; // P2 holds R2
    P2 --> R1_req;

    R2 --> P3_holds;
    P3_holds --> P3; // P3 holds R2 (other instance)


    classDef process fill:#f9f,stroke:#333,stroke-width:2px;
    classDef resource fill:#9f9,stroke:#333,stroke-width:2px,rx:5px,ry:5px;

    class P1,P2,P3 process;
    class R1,R2 resource;

    // Representing request/assignment for actual graph
    P1 ---R1_req---> R2;
    P2 ---R2_req---> R1;
    R1 ---R1_assign---> P1;
    R2 ---R2_assign1---> P2;
    R2 ---R2_assign2---> P3;

    // Simplified deadlock graph, assuming P1 holds R1, P2 holds R2
    // and P1 requests R2, P2 requests R1

    P1_node --> R1_instance_node;
    R1_instance_node --> P2_node;
    P2_node --> R2_instance_node;
    R2_instance_node --> P1_node;

    style R1_instance_node fill:#9f9,stroke:#333,stroke-width:2px,rx:5px,ry:5px;
    style R2_instance_node fill:#9f9,stroke:#333,stroke-width:2px,rx:5px,ry:5px;
```

Let's simplify and draw the graph textually for the given scenario:

**Nodes:**

  * Processes: P1, P2, P3 (circles)
  * Resource Types: R1 (1 instance), R2 (2 instances) (rectangles with dots)

**Edges:**

  * P1 currently holds R1: $R\_1 \\rightarrow P\_1$ (Assignment edge from an R1 instance to P1)
  * P2 currently holds an instance of R2: $R\_2 \\rightarrow P\_2$ (Assignment edge from one R2 instance to P2)
  * P3 currently holds an instance of R2: $R\_2 \\rightarrow P\_3$ (Assignment edge from the other R2 instance to P3)
  * P1 requests R2: $P\_1 \\rightarrow R\_2$ (Request edge from P1 to R2)
  * P2 requests R1: $P\_2 \\rightarrow R\_1$ (Request edge from P2 to R1)

**Resulting Graph (conceptual):**

  * A cycle exists: $P\_1 \\rightarrow R\_2 \\rightarrow P\_2 \\rightarrow R\_1 \\rightarrow P\_1$

  * **Analysis:**

      * $P\_1$ holds $R\_1$ and requests $R\_2$.
      * $P\_2$ holds an instance of $R\_2$ and requests $R\_1$.
      * $P\_3$ holds an instance of $R\_2$.

    Since $P\_1$ needs $R\_2$ (which is held by $P\_2$ and $P\_3$) and $P\_2$ needs $R\_1$ (held by $P\_1$), there is a circular wait condition.
    $P\_1$ is waiting for $R\_2$. $P\_2$ is waiting for $R\_1$.
    $R\_1$ is currently held by $P\_1$.
    $R\_2$ has both its instances held by $P\_2$ and $P\_3$.

    Yes, there is a cycle: $P\_1 \\rightarrow R\_2 \\rightarrow P\_2 \\rightarrow R\_1 \\rightarrow P\_1$.
    Since $R\_1$ has only one instance and it's involved in the cycle, and $R\_2$ has both instances allocated to $P\_2$ and $P\_3$, it means $P\_1$ cannot get $R\_2$, and $P\_2$ cannot get $R\_1$. Therefore, this scenario **results in a deadlock**.

### 9\. a) What is deadlock ? Explain the necessary conditions for deadlock to occur.

**Deadlock:**

A **deadlock** is a situation in a multiprogramming environment where two or more processes are permanently blocked, waiting indefinitely for resources that are held by other processes, which are themselves waiting for resources held by the first set of processes. It's a circular waiting condition where no process can proceed because each is waiting for a resource held by another process in the same set.

Imagine a traffic jam where four cars are stuck at a four-way intersection, each waiting for the other to move. No car can move because each is blocking the path of another. This is analogous to a deadlock in an operating system.

**Necessary Conditions for Deadlock to Occur:**

For a deadlock to occur, all four of the following conditions must hold true simultaneously:

1.  **Mutual Exclusion:** At least one resource must be held in a non-sharable mode. This means that only one process at a time can use the resource. If another process requests that resource, the requesting process must be delayed until the resource has been released. (e.g., a printer cannot be shared simultaneously by two processes; they must take turns).
2.  **Hold and Wait:** A process must be holding at least one resource and simultaneously waiting to acquire additional resources that are currently being held by other processes. (e.g., a process holds a scanner and is waiting for a printer).
3.  **No Preemption:** Resources cannot be preempted (forcibly taken away) from a process that is holding them. A resource can only be released voluntarily by the process that is holding it, after that process has completed its task with that resource. (e.g., you cannot forcibly take a printed page from a printer before it's done).
4.  **Circular Wait:** A set of processes ${P\_0, P\_1, \\dots, P\_n}$ must exist such that $P\_0$ is waiting for a resource held by $P\_1$, $P\_1$ is waiting for a resource held by $P\_2$, ..., $P\_{n-1}$ is waiting for a resource held by $P\_n$, and $P\_n$ is waiting for a resource held by $P\_0$. This forms a closed chain of waiting processes, where each process in the chain is waiting for a resource held by the next process in the chain.

If any one of these four conditions is not met, a deadlock cannot occur. Therefore, deadlock prevention strategies often focus on negating one or more of these conditions.

### b) Explain Banker's Algorithm.

The **Banker's Algorithm** is a deadlock avoidance algorithm. It determines whether a state is "safe" (i.e., a state where the system can allocate resources to each process in some order and still avoid a deadlock) or "unsafe" when a process requests resources. The algorithm is named after its analogy with a banker who wisely manages money to avoid bankruptcy; similarly, the OS manages resources to avoid deadlock.

It is more complex than deadlock detection or prevention algorithms but less restrictive. It allows processes to request resources as needed, as long as the system remains in a safe state.

**Key Concepts and Data Structures:**

The Banker's algorithm requires knowing three key pieces of information about each process:

1.  **Available:** A vector of length 'm' (number of resource types) indicating the number of available instances of each resource type. `Available[j] = k` means 'k' instances of resource type $R\_j$ are available.
2.  **Max:** An 'n x m' matrix defining the maximum demand of each process for each resource type. `Max[i][j] = k` means process $P\_i$ may request at most 'k' instances of resource type $R\_j$.
3.  **Allocation:** An 'n x m' matrix defining the number of resources of each type currently allocated to each process. `Allocation[i][j] = k` means process $P\_i$ is currently allocated 'k' instances of resource type $R\_j$.
4.  **Need:** An 'n x m' matrix indicating the remaining resources needed by each process. `Need[i][j] = Max[i][j] - Allocation[i][j]`.

**Safety Algorithm (How to check if a state is safe):**

1.  Initialize `Work = Available` and `Finish[i] = false` for all processes $P\_i$.
2.  Find an index `i` such that `Finish[i] == false` and `Need[i] <= Work`. (This means process $P\_i$ can be completed with the currently available resources).
3.  If no such `i` exists, go to step 4.
4.  If such an `i` is found:
      * `Work = Work + Allocation[i]` (Simulate process $P\_i$ finishing and releasing its resources).
      * `Finish[i] = true`.
      * Go to step 2.
5.  If `Finish[i] == true` for all `i`, then the system is in a safe state. Otherwise, it is in an unsafe state.

**Resource-Request Algorithm (How to handle a request):**

When process $P\_i$ requests `Request[i]` instances of a resource type:

1.  If `Request[i] > Need[i]`, then error (process has requested more than its declared maximum).
2.  If `Request[i] > Available`, then process $P\_i$ must wait (resources are not available).
3.  Otherwise (resources are available):
      * Pretend to allocate the requested resources:
          * `Available = Available - Request[i]`
          * `Allocation[i] = Allocation[i] + Request[i]`
          * `Need[i] = Need[i] - Request[i]`
      * Now, run the **Safety Algorithm** on this new (hypothetical) state.
      * If the state is safe, grant the request.
      * If the state is unsafe, revoke the pretended allocation (restore previous state) and make $P\_i$ wait.

**Example:**

Consider a system with 5 processes ($P\_0$ to $P\_4$) and 3 resource types A, B, C.

  * Resource A: 10 instances
  * Resource B: 5 instances
  * Resource C: 7 instances

**Snapshot at Time T0:**

| Process | Allocation (A B C) | Max (A B C) | Need (A B C) |
| :------ | :----------------- | :---------- | :----------- |
| P0      | 0 1 0              | 7 5 3       | 7 4 3        |
| P1      | 2 0 0              | 3 2 2       | 1 2 2        |
| P2      | 3 0 2              | 9 0 2       | 6 0 0        |
| P3      | 2 1 1              | 2 2 2       | 0 1 1        |
| P4      | 0 0 2              | 4 3 3       | 4 3 1        |

**Available (A B C): 3 3 2** (Total: A:10, B:5, C:7. Allocated: A:7, B:2, C:5. Available = Total - Allocated = A:(10-7)=3, B:(5-2)=3, C:(7-5)=2)

**Is the system in a safe state?**

1.  `Work = (3, 3, 2)`, `Finish = {F, F, F, F, F}`

2.  **Find P such that Need \<= Work:**

      * P0: (7,4,3) \> (3,3,2) - No
      * P1: (1,2,2) \<= (3,3,2) - Yes\! (Choose P1)

3.  **Execute P1:**

      * `Work = Work + Allocation[P1] = (3,3,2) + (2,0,0) = (5,3,2)`
      * `Finish[P1] = T`
      * `Finish = {F, T, F, F, F}`

4.  **Find next P such that Need \<= Work:**

      * P0: (7,4,3) \> (5,3,2) - No
      * P2: (6,0,0) \> (5,3,2) - No
      * P3: (0,1,1) \<= (5,3,2) - Yes\! (Choose P3)

5.  **Execute P3:**

      * `Work = Work + Allocation[P3] = (5,3,2) + (2,1,1) = (7,4,3)`
      * `Finish[P3] = T`
      * `Finish = {F, T, F, T, F}`

6.  **Find next P such that Need \<= Work:**

      * P0: (7,4,3) \<= (7,4,3) - Yes\! (Choose P0)

7.  **Execute P0:**

      * `Work = Work + Allocation[P0] = (7,4,3) + (0,1,0) = (7,5,3)`
      * `Finish[P0] = T`
      * `Finish = {T, T, F, T, F}`

8.  **Find next P such that Need \<= Work:**

      * P2: (6,0,0) \<= (7,5,3) - Yes\! (Choose P2)

9.  **Execute P2:**

      * `Work = Work + Allocation[P2] = (7,5,3) + (3,0,2) = (10,5,5)`
      * `Finish[P2] = T`
      * `Finish = {T, T, T, T, F}`

10. **Find next P such that Need \<= Work:**

      * P4: (4,3,1) \<= (10,5,5) - Yes\! (Choose P4)

11. **Execute P4:**

      * `Work = Work + Allocation[P4] = (10,5,5) + (0,0,2) = (10,5,7)`
      * `Finish[P4] = T`
      * `Finish = {T, T, T, T, T}`

Since all `Finish` values are `true`, the system is in a **safe state**. A possible safe sequence is `<P1, P3, P0, P2, P4>`.

**Advantages:**

  * Avoids deadlock by ensuring the system always remains in a safe state.
  * Allows for more concurrency than strict deadlock prevention schemes.

**Disadvantages:**

  * Requires processes to declare their maximum resource needs in advance, which is often not practical.
  * Requires a fixed number of processes and resources.
  * Resource instances are static (cannot be added or removed dynamically).
  * High overhead due to frequent execution of the safety algorithm.
  * Low resource utilization, as it may deny a request even if resources are currently available if it leads to an unsafe state.
